{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# karate club"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path = 'data/zachary/out.ucidata-zachary'\n",
    "x = pd.read_csv(path,sep=' ', header=None)\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "x = x.to_numpy()\n",
    "weight = np.ones(x.shape[0])\n",
    "max_val = np.amax(x)\n",
    "nodes = np.unique(x)\n",
    "nodes = np.array(list(map(lambda t:t-1, nodes)))\n",
    "\n",
    "adj = sparse.coo_matrix((weight, (x[:,0], x[:,1])), shape=[max_val+1, max_val+1])\n",
    "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "adj = adj.todense() # adj\n",
    "# print(adj)\n",
    "path = 'data/zachary/label.txt'\n",
    "label = pd.read_csv(path, sep=' ', header=None).to_numpy()\n",
    "node_label = {n:[] for n in nodes}\n",
    "\n",
    "for l,i in zip(label[:,1].tolist(), label[:,0].tolist()):\n",
    "    node_label[i-1] = l\n",
    "\n",
    "\n",
    "labels = list(map(node_label.get, nodes))\n",
    "\n",
    "# print(nodes)\n",
    "# print(labels)\n",
    "# print(node_label)\n",
    "# print(x.min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "path = 'data/zachary/karate.gml'\n",
    "G = nx.read_gml(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.conv1 = GCNConv(len(nodes), 4)\n",
    "        self.conv2 = GCNConv(4,2) \n",
    "    def forward(self):\n",
    "        import pandas as pd\n",
    "        path = 'data/zachary/out.ucidata-zachary'\n",
    "        edges = pd.read_csv(path,sep=' ', header=None)\n",
    "        import numpy as np\n",
    "        from scipy import sparse\n",
    "        edges = edges.to_numpy()\n",
    "        # (2, number of edges)\n",
    "        edges_index = np.transpose(edges, (1,0)).flatten()\n",
    "        edges_index = np.array(list(map(lambda t:t-1, edges_index))).reshape(2,-1)\n",
    "        edges_index = torch.tensor(edges_index, dtype=torch.long)\n",
    "        # (len(nodes), len(nodes))\n",
    "        x = np.identity(len(nodes))\n",
    "#         print(f\"x.shape ={x.shape}\")\n",
    "#         print(f\"max_val in edges = {np.amax(edges_index)})\n",
    "        x = torch.tensor(x, dtype=torch.long).type(torch.float)\n",
    "\n",
    "        x = F.tanh(self.conv1(x,edges_index))\n",
    "        x = F.tanh(self.conv2(x,edges_index))\n",
    "#         x = F.tanh(self.conv3(x,edges_index))\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\awannaphasch2016\\AppData\\Local\\Continuum\\anaconda3\\envs\\pytorch_python3.7\\lib\\site-packages\\torch\\nn\\functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Assertion `cur_target >= 0 && cur_target < n_classes' failed.  at ..\\aten\\src\\THNN/generic/ClassNLLCriterion.c:92",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-0d3532b489c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mloss_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[0mloss_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\pytorch_python3.7\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   1873\u001b[0m         \u001b[1;31m# exit()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1874\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1875\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1876\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1877\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Assertion `cur_target >= 0 && cur_target < n_classes' failed.  at ..\\aten\\src\\THNN/generic/ClassNLLCriterion.c:92"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "path = 'data/zachary/out.ucidata-zachary'\n",
    "x = pd.read_csv(path,sep=' ', header=None)\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "x = x.to_numpy()\n",
    "weight = np.ones(x.shape[0])\n",
    "max_val = np.amax(x)\n",
    "nodes = np.unique(x)\n",
    "nodes = np.array(list(map(lambda t:t-1, nodes)))\n",
    "\n",
    "adj = sparse.coo_matrix((weight, (x[:,0], x[:,1])), shape=[max_val+1, max_val+1])\n",
    "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "adj = adj.todense() # adj\n",
    "# print(adj)\n",
    "path = 'data/zachary/label.txt'\n",
    "label = pd.read_csv(path, sep=' ', header=None).to_numpy()\n",
    "node_label = {n:[] for n in nodes}\n",
    "\n",
    "for l,i in zip(label[:,1].tolist(), label[:,0].tolist()):\n",
    "    node_label[i-1] = l\n",
    "\n",
    "\n",
    "labels = list(map(node_label.get, nodes))\n",
    "\n",
    "model = Net()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for i in range(300):\n",
    "    model.train()\n",
    "    if i ==0:\n",
    "        import matplotlib.pyplot as plt\n",
    "        from sklearn.manifold import TSNE\n",
    "        y = model().detach().numpy()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    loss_output = F.nll_loss(model(),labels)\n",
    "    loss_output.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    pred = model().max(1)[1]\n",
    "    acc = pred.eq(labels).sum().item()/ len(nodes)\n",
    "#     if i == 299:\n",
    "#         print(f\"epcoh ={i}; train_acc = {acc}\")\n",
    "    print(f\"epcoh ={i}; train_acc = {acc}\")\n",
    "\n",
    "print('plotting...')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "y = model().detach().numpy()\n",
    "emb = TSNE(n_components=2).fit_transform(y)\n",
    "colors=pred.detach().numpy().tolist()\n",
    "print(f'labels {labels}')\n",
    "print(f'color {colors}')\n",
    "plt.scatter(emb[:,0], emb[:,1],c=colors)\n",
    "plt.title('GCN karate club')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\awannaphasch2016\\AppData\\Local\\Continuum\\anaconda3\\envs\\pytorch_python3.7\\lib\\site-packages\\torch\\nn\\functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9919, -0.7210, -1.7323, -1.6072],\n",
       "        [-1.8599, -0.8105, -1.5498, -1.6745],\n",
       "        [-1.8647, -0.8996, -1.3495, -1.7207],\n",
       "        [-2.2559, -0.4690, -1.7973, -2.2646],\n",
       "        [-0.3913, -2.2250, -2.1872, -2.2682],\n",
       "        [-0.5206, -1.9422, -1.9936, -2.0694],\n",
       "        [-0.3408, -2.3408, -2.3404, -2.3408],\n",
       "        [-2.3408, -0.3408, -2.3407, -2.3408],\n",
       "        [-2.2752, -2.1908, -2.1120, -0.4089],\n",
       "        [-2.3389, -2.3316, -0.3433, -2.3329],\n",
       "        [-0.3408, -2.3408, -2.3408, -2.3408],\n",
       "        [-2.3368, -0.3418, -2.3366, -2.3415],\n",
       "        [-2.3387, -0.3410, -2.3410, -2.3410],\n",
       "        [-2.3411, -0.3415, -2.3343, -2.3414],\n",
       "        [-2.2999, -2.2785, -2.2817, -0.3636],\n",
       "        [-2.2921, -2.2700, -2.2815, -0.3660],\n",
       "        [-0.3408, -2.3408, -2.3408, -2.3408],\n",
       "        [-2.3404, -0.3418, -2.3323, -2.3418],\n",
       "        [-2.2901, -2.2709, -2.2791, -0.3665],\n",
       "        [-2.3324, -0.3471, -2.2979, -2.3461],\n",
       "        [-2.2960, -2.2795, -2.2814, -0.3640],\n",
       "        [-2.3406, -0.3412, -2.3368, -2.3412],\n",
       "        [-2.2966, -2.2736, -2.2800, -0.3650],\n",
       "        [-1.9603, -1.6193, -1.7912, -0.7045],\n",
       "        [-2.1614, -1.8549, -0.4778, -2.2235],\n",
       "        [-2.3386, -2.3467, -0.3467, -2.2946],\n",
       "        [-2.2952, -0.3915, -2.2083, -2.1774],\n",
       "        [-2.3443, -2.3455, -0.3455, -2.2981],\n",
       "        [-2.3305, -2.2593, -0.3600, -2.2961],\n",
       "        [-2.3346, -2.3044, -2.3390, -0.3469],\n",
       "        [-2.3398, -2.3357, -2.3386, -0.3419],\n",
       "        [-2.3396, -2.3409, -0.3409, -2.3409],\n",
       "        [-2.3408, -2.3408, -2.3408, -0.3408],\n",
       "        [-2.3408, -2.3408, -2.3408, -0.3408]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GCN with base line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 001, Train: 0.1429, Val: 0.1220, Test: 0.1320\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 002, Train: 0.1857, Val: 0.1240, Test: 0.1390\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 003, Train: 0.6286, Val: 0.3700, Test: 0.3940\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 004, Train: 0.7429, Val: 0.4120, Test: 0.4230\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 005, Train: 0.7714, Val: 0.4360, Test: 0.4610\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 006, Train: 0.8071, Val: 0.4880, Test: 0.5330\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 007, Train: 0.8071, Val: 0.5540, Test: 0.5750\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 008, Train: 0.8214, Val: 0.6080, Test: 0.6140\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 009, Train: 0.8500, Val: 0.6420, Test: 0.6310\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 010, Train: 0.8643, Val: 0.6420, Test: 0.6310\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 011, Train: 0.8714, Val: 0.6420, Test: 0.6310\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 012, Train: 0.8500, Val: 0.6420, Test: 0.6310\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 013, Train: 0.8643, Val: 0.6420, Test: 0.6310\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 014, Train: 0.8500, Val: 0.6420, Test: 0.6310\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 015, Train: 0.8714, Val: 0.6420, Test: 0.6310\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 016, Train: 0.8929, Val: 0.6460, Test: 0.6210\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 017, Train: 0.8929, Val: 0.6640, Test: 0.6380\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 018, Train: 0.9000, Val: 0.6680, Test: 0.6530\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 019, Train: 0.9143, Val: 0.6680, Test: 0.6530\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 020, Train: 0.9214, Val: 0.6680, Test: 0.6530\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 021, Train: 0.9143, Val: 0.6680, Test: 0.6530\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 022, Train: 0.9214, Val: 0.6680, Test: 0.6530\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 023, Train: 0.9286, Val: 0.6680, Test: 0.6530\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 024, Train: 0.9357, Val: 0.6720, Test: 0.6910\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 025, Train: 0.9357, Val: 0.6920, Test: 0.7050\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 026, Train: 0.9286, Val: 0.6980, Test: 0.7120\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 027, Train: 0.9286, Val: 0.7040, Test: 0.7170\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 028, Train: 0.9357, Val: 0.7200, Test: 0.7330\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 029, Train: 0.9357, Val: 0.7260, Test: 0.7400\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 030, Train: 0.9357, Val: 0.7260, Test: 0.7400\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 031, Train: 0.9357, Val: 0.7380, Test: 0.7440\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 032, Train: 0.9429, Val: 0.7420, Test: 0.7490\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 033, Train: 0.9429, Val: 0.7480, Test: 0.7510\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 034, Train: 0.9500, Val: 0.7600, Test: 0.7530\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 035, Train: 0.9500, Val: 0.7600, Test: 0.7530\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 036, Train: 0.9500, Val: 0.7600, Test: 0.7530\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 037, Train: 0.9500, Val: 0.7600, Test: 0.7530\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 038, Train: 0.9571, Val: 0.7600, Test: 0.7530\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 039, Train: 0.9571, Val: 0.7600, Test: 0.7530\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 040, Train: 0.9571, Val: 0.7600, Test: 0.7530\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 041, Train: 0.9500, Val: 0.7600, Test: 0.7530\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 042, Train: 0.9500, Val: 0.7600, Test: 0.7530\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 043, Train: 0.9429, Val: 0.7600, Test: 0.7530\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 044, Train: 0.9429, Val: 0.7600, Test: 0.7530\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 045, Train: 0.9500, Val: 0.7600, Test: 0.7530\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 046, Train: 0.9500, Val: 0.7600, Test: 0.7530\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 047, Train: 0.9571, Val: 0.7600, Test: 0.7530\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 048, Train: 0.9571, Val: 0.7600, Test: 0.7530\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 049, Train: 0.9571, Val: 0.7600, Test: 0.7530\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 050, Train: 0.9571, Val: 0.7600, Test: 0.7530\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 051, Train: 0.9571, Val: 0.7640, Test: 0.7890\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 052, Train: 0.9571, Val: 0.7640, Test: 0.7890\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 053, Train: 0.9571, Val: 0.7640, Test: 0.7890\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 054, Train: 0.9571, Val: 0.7640, Test: 0.7890\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 055, Train: 0.9571, Val: 0.7640, Test: 0.7890\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 056, Train: 0.9571, Val: 0.7660, Test: 0.7880\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 057, Train: 0.9571, Val: 0.7660, Test: 0.7880\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 058, Train: 0.9571, Val: 0.7700, Test: 0.7930\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 059, Train: 0.9571, Val: 0.7780, Test: 0.7930\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 060, Train: 0.9571, Val: 0.7780, Test: 0.7930\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 061, Train: 0.9643, Val: 0.7780, Test: 0.7930\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 062, Train: 0.9643, Val: 0.7780, Test: 0.7930\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 063, Train: 0.9643, Val: 0.7780, Test: 0.7930\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 064, Train: 0.9643, Val: 0.7780, Test: 0.7930\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 065, Train: 0.9643, Val: 0.7780, Test: 0.7930\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 066, Train: 0.9643, Val: 0.7780, Test: 0.7930\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 067, Train: 0.9643, Val: 0.7780, Test: 0.7930\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 068, Train: 0.9643, Val: 0.7780, Test: 0.7930\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 069, Train: 0.9643, Val: 0.7780, Test: 0.7930\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 070, Train: 0.9643, Val: 0.7780, Test: 0.7930\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 071, Train: 0.9571, Val: 0.7780, Test: 0.7930\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 072, Train: 0.9571, Val: 0.7780, Test: 0.7930\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 073, Train: 0.9571, Val: 0.7780, Test: 0.7930\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 074, Train: 0.9571, Val: 0.7780, Test: 0.7930\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 075, Train: 0.9571, Val: 0.7780, Test: 0.7930\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 076, Train: 0.9571, Val: 0.7780, Test: 0.7930\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 077, Train: 0.9643, Val: 0.7780, Test: 0.7930\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 078, Train: 0.9571, Val: 0.7780, Test: 0.7930\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 079, Train: 0.9571, Val: 0.7780, Test: 0.7930\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 080, Train: 0.9643, Val: 0.7780, Test: 0.7930\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 081, Train: 0.9643, Val: 0.7780, Test: 0.7930\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 082, Train: 0.9643, Val: 0.7780, Test: 0.7930\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 083, Train: 0.9714, Val: 0.7780, Test: 0.7930\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 084, Train: 0.9714, Val: 0.7780, Test: 0.7930\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 085, Train: 0.9714, Val: 0.7780, Test: 0.7930\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 086, Train: 0.9714, Val: 0.7780, Test: 0.7930\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 087, Train: 0.9786, Val: 0.7800, Test: 0.8030\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 088, Train: 0.9786, Val: 0.7880, Test: 0.8010\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 089, Train: 0.9786, Val: 0.7900, Test: 0.8050\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 090, Train: 0.9786, Val: 0.7900, Test: 0.8050\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 091, Train: 0.9786, Val: 0.7900, Test: 0.8050\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 092, Train: 0.9786, Val: 0.7900, Test: 0.8050\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 093, Train: 0.9714, Val: 0.7900, Test: 0.8050\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 094, Train: 0.9714, Val: 0.7900, Test: 0.8050\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 095, Train: 0.9714, Val: 0.7900, Test: 0.8050\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 096, Train: 0.9714, Val: 0.7900, Test: 0.8050\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 097, Train: 0.9714, Val: 0.7900, Test: 0.8050\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 098, Train: 0.9786, Val: 0.7900, Test: 0.8050\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 099, Train: 0.9786, Val: 0.7900, Test: 0.8050\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 100, Train: 0.9786, Val: 0.7940, Test: 0.8090\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 101, Train: 0.9786, Val: 0.7940, Test: 0.8090\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 102, Train: 0.9786, Val: 0.7940, Test: 0.8090\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 103, Train: 0.9786, Val: 0.7940, Test: 0.8090\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 104, Train: 0.9786, Val: 0.7940, Test: 0.8090\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 105, Train: 0.9786, Val: 0.7940, Test: 0.8090\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 106, Train: 0.9786, Val: 0.7940, Test: 0.8090\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 107, Train: 0.9786, Val: 0.7940, Test: 0.8090\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 108, Train: 0.9786, Val: 0.7940, Test: 0.8090\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 109, Train: 0.9786, Val: 0.7940, Test: 0.8090\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 110, Train: 0.9786, Val: 0.7940, Test: 0.8090\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 111, Train: 0.9786, Val: 0.7940, Test: 0.8090\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 112, Train: 0.9786, Val: 0.7940, Test: 0.8090\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 113, Train: 0.9857, Val: 0.7940, Test: 0.8090\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 114, Train: 0.9857, Val: 0.8000, Test: 0.8180\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 115, Train: 0.9857, Val: 0.8000, Test: 0.8180\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 116, Train: 0.9857, Val: 0.8000, Test: 0.8180\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 117, Train: 0.9857, Val: 0.8000, Test: 0.8180\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 118, Train: 0.9857, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 119, Train: 0.9857, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 120, Train: 0.9786, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 121, Train: 0.9786, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 122, Train: 0.9786, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 123, Train: 0.9857, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 124, Train: 0.9857, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 125, Train: 0.9857, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 126, Train: 0.9857, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 127, Train: 0.9857, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 128, Train: 0.9857, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 129, Train: 0.9857, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 130, Train: 0.9786, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 131, Train: 0.9786, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 132, Train: 0.9786, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 133, Train: 0.9786, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 134, Train: 0.9857, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 135, Train: 0.9857, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 136, Train: 0.9857, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 137, Train: 0.9857, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 138, Train: 0.9857, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 139, Train: 0.9857, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 140, Train: 0.9857, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 141, Train: 0.9857, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 142, Train: 0.9857, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 143, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 144, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 145, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 146, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 147, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 148, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 149, Train: 0.9857, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 150, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 151, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 152, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 153, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 154, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 155, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 156, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 157, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 158, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 159, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 160, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 161, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 162, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 163, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 164, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 165, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 166, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 167, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 168, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 169, Train: 0.9857, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 170, Train: 0.9857, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 171, Train: 0.9857, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 172, Train: 0.9857, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 173, Train: 0.9857, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 174, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 175, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 176, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 177, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 178, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 179, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 180, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 181, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 182, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 183, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 184, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 185, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 186, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 187, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 188, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 189, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 190, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 191, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 192, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 193, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 194, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 195, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 196, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 197, Train: 0.9929, Val: 0.8060, Test: 0.8190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 198, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 199, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "tensor(0.0435)\n",
      "tensor(0.0435)\n",
      "Epoch: 200, Train: 0.9929, Val: 0.8060, Test: 0.8190\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, ChebConv  # noqa\n",
    "\n",
    "\n",
    "# dataset = 'Cora'\n",
    "# path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', dataset)\n",
    "\n",
    "# dataset = Planetoid(path, dataset, T.NormalizeFeatures())\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora', transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_features, 16, cached=True)\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes, cached=True)\n",
    "        # self.conv1 = ChebConv(data.num_features, 16, K=2)\n",
    "        # self.conv2 = ChebConv(16, data.num_features, K=2)\n",
    "\n",
    "    def forward(self):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "#         print(np.nonzero(x[1,:].numpy()))\n",
    "        print(x[1,19])\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model, data = Net().to(device), data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "#     print(np.nonzero(data.train_mask.numpy())[0].shape)\n",
    "    F.nll_loss(model()[data.train_mask], data.y[data.train_mask]).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    logits, accs = model(), []\n",
    "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "        pred = logits[mask].max(1)[1]\n",
    "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        accs.append(acc)\n",
    "    return accs\n",
    "\n",
    "\n",
    "best_val_acc = test_acc = 0\n",
    "for epoch in range(1, 201):\n",
    "    train()\n",
    "    train_acc, val_acc, tmp_test_acc = test()\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    log = 'Epoch: {:03d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "    print(log.format(epoch, train_acc, best_val_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>26</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>29</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>9</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>15</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>16</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>19</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>21</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>23</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>24</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>30</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>31</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>32</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>9</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>10</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>14</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>15</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>16</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>19</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>20</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>21</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>23</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>24</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>27</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>28</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>29</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>30</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>31</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>32</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0    1   2\n",
       "1    1   3\n",
       "2    2   3\n",
       "3    1   4\n",
       "4    2   4\n",
       "5    3   4\n",
       "6    1   5\n",
       "7    1   6\n",
       "8    1   7\n",
       "9    5   7\n",
       "10   6   7\n",
       "11   1   8\n",
       "12   2   8\n",
       "13   3   8\n",
       "14   4   8\n",
       "15   1   9\n",
       "16   3   9\n",
       "17   3  10\n",
       "18   1  11\n",
       "19   5  11\n",
       "20   6  11\n",
       "21   1  12\n",
       "22   1  13\n",
       "23   4  13\n",
       "24   1  14\n",
       "25   2  14\n",
       "26   3  14\n",
       "27   4  14\n",
       "28   6  17\n",
       "29   7  17\n",
       "..  ..  ..\n",
       "48  26  32\n",
       "49  29  32\n",
       "50   3  33\n",
       "51   9  33\n",
       "52  15  33\n",
       "53  16  33\n",
       "54  19  33\n",
       "55  21  33\n",
       "56  23  33\n",
       "57  24  33\n",
       "58  30  33\n",
       "59  31  33\n",
       "60  32  33\n",
       "61   9  34\n",
       "62  10  34\n",
       "63  14  34\n",
       "64  15  34\n",
       "65  16  34\n",
       "66  19  34\n",
       "67  20  34\n",
       "68  21  34\n",
       "69  23  34\n",
       "70  24  34\n",
       "71  27  34\n",
       "72  28  34\n",
       "73  29  34\n",
       "74  30  34\n",
       "75  31  34\n",
       "76  32  34\n",
       "77  33  34\n",
       "\n",
       "[78 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(array([  19,   88,  149,  212,  233,  332,  336,  359,  472,  507,  548,\n",
    "        687,  763,  808,  889, 1058, 1177, 1254, 1257, 1262, 1332, 1339,\n",
    "       1349], dtype=int64),)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4], dtype=int64), array([0, 1, 2, 3, 4], dtype=int64))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.identity(5)\n",
    "np.nonzero(x[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'super' has no attribute 'foo'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-a8b6a77be706>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfee\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mme\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-a8b6a77be706>\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfoo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfoo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mchild_right\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfoo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mchild_left\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfoo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'super' has no attribute 'foo'"
     ]
    }
   ],
   "source": [
    "class parent:\n",
    "    def foo(self):\n",
    "        print('parent')\n",
    "    def fee(self):\n",
    "        prin(\"parent_fee\")\n",
    "class child_left(parent):\n",
    "    def foo(self):\n",
    "        print('child_left')\n",
    "class child_right(parent):\n",
    "    def foo(self):\n",
    "        print('child_right')\n",
    "class me(child_left, child_right):\n",
    "    def foo(self):\n",
    "        print(\"me\")\n",
    "    def show(self):\n",
    "        self.foo()\n",
    "        super.foo()\n",
    "        child_right.foo()\n",
    "        child_left.foo()\n",
    "        self.fee()\n",
    "        super.fee()\n",
    "        \n",
    "m = me()\n",
    "m.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
