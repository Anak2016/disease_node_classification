{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x13f9e67b898>,\n",
       "  <matplotlib.axis.XTick at 0x13f9e49eda0>,\n",
       "  <matplotlib.axis.XTick at 0x13f9ddd7358>],\n",
       " <a list of 3 Text xticklabel objects>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = np.arange(3)\n",
    "print(x)\n",
    "plt.bar([1,0,2], height= [1,2,3])\n",
    "plt.xticks(x, ['a','b','c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "class C(object):\n",
    "    def __init__(self, i):\n",
    "        self.i = i\n",
    "    def __getstate__(self):\n",
    "        return {'i': self.i}\n",
    "assert pickle.loads(pickle.dumps(C(1), -1)).i == 1\n",
    "pickle.loads(pickle.dumps(C(1), -1)).i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n",
      "Epoch: 001, Train: 0.2429, Val: 0.2240, Test: 0.2010\n",
      "Epoch: 002, Train: 0.5643, Val: 0.3680, Test: 0.3450\n",
      "Epoch: 003, Train: 0.6429, Val: 0.4400, Test: 0.4430\n",
      "Epoch: 004, Train: 0.6071, Val: 0.4000, Test: 0.3930\n",
      "Epoch: 005, Train: 0.6143, Val: 0.3740, Test: 0.3740\n",
      "Epoch: 006, Train: 0.6714, Val: 0.4280, Test: 0.4170\n",
      "Epoch: 007, Train: 0.7071, Val: 0.4520, Test: 0.4250\n",
      "Epoch: 008, Train: 0.7000, Val: 0.4340, Test: 0.4290\n",
      "Epoch: 009, Train: 0.6643, Val: 0.4220, Test: 0.4270\n",
      "Epoch: 010, Train: 0.6929, Val: 0.4440, Test: 0.4380\n",
      "Epoch: 011, Train: 0.7071, Val: 0.4460, Test: 0.4340\n",
      "Epoch: 012, Train: 0.7429, Val: 0.4560, Test: 0.4420\n",
      "Epoch: 013, Train: 0.7929, Val: 0.4960, Test: 0.4850\n",
      "Epoch: 014, Train: 0.7929, Val: 0.5080, Test: 0.5030\n",
      "Epoch: 015, Train: 0.8286, Val: 0.5360, Test: 0.5390\n",
      "Epoch: 016, Train: 0.8500, Val: 0.5800, Test: 0.5850\n",
      "Epoch: 017, Train: 0.8857, Val: 0.6380, Test: 0.6360\n",
      "Epoch: 018, Train: 0.8929, Val: 0.6760, Test: 0.6810\n",
      "Epoch: 019, Train: 0.8929, Val: 0.7180, Test: 0.7140\n",
      "Epoch: 020, Train: 0.9143, Val: 0.7460, Test: 0.7480\n",
      "Epoch: 021, Train: 0.9286, Val: 0.7660, Test: 0.7640\n",
      "Epoch: 022, Train: 0.9286, Val: 0.7760, Test: 0.7730\n",
      "Epoch: 023, Train: 0.9214, Val: 0.7740, Test: 0.7780\n",
      "Epoch: 024, Train: 0.9286, Val: 0.7720, Test: 0.7870\n",
      "Epoch: 025, Train: 0.9500, Val: 0.7740, Test: 0.7890\n",
      "Epoch: 026, Train: 0.9500, Val: 0.7700, Test: 0.7820\n",
      "Epoch: 027, Train: 0.9500, Val: 0.7760, Test: 0.7770\n",
      "Epoch: 028, Train: 0.9500, Val: 0.7680, Test: 0.7720\n",
      "Epoch: 029, Train: 0.9429, Val: 0.7660, Test: 0.7720\n",
      "Epoch: 030, Train: 0.9500, Val: 0.7660, Test: 0.7630\n",
      "Epoch: 031, Train: 0.9500, Val: 0.7620, Test: 0.7610\n",
      "Epoch: 032, Train: 0.9500, Val: 0.7560, Test: 0.7540\n",
      "Epoch: 033, Train: 0.9429, Val: 0.7560, Test: 0.7540\n",
      "Epoch: 034, Train: 0.9500, Val: 0.7680, Test: 0.7640\n",
      "Epoch: 035, Train: 0.9571, Val: 0.7840, Test: 0.7730\n",
      "Epoch: 036, Train: 0.9571, Val: 0.7920, Test: 0.7820\n",
      "Epoch: 037, Train: 0.9571, Val: 0.7960, Test: 0.7910\n",
      "Epoch: 038, Train: 0.9643, Val: 0.7980, Test: 0.7990\n",
      "Epoch: 039, Train: 0.9643, Val: 0.8000, Test: 0.8040\n",
      "Epoch: 040, Train: 0.9571, Val: 0.7980, Test: 0.8100\n",
      "Epoch: 041, Train: 0.9571, Val: 0.8000, Test: 0.8120\n",
      "Epoch: 042, Train: 0.9571, Val: 0.7920, Test: 0.8160\n",
      "Epoch: 043, Train: 0.9643, Val: 0.7880, Test: 0.8180\n",
      "Epoch: 044, Train: 0.9500, Val: 0.7900, Test: 0.8250\n",
      "Epoch: 045, Train: 0.9500, Val: 0.7900, Test: 0.8230\n",
      "Epoch: 046, Train: 0.9429, Val: 0.7900, Test: 0.8170\n",
      "Epoch: 047, Train: 0.9429, Val: 0.7880, Test: 0.8050\n",
      "Epoch: 048, Train: 0.9429, Val: 0.7920, Test: 0.8050\n",
      "Epoch: 049, Train: 0.9429, Val: 0.7940, Test: 0.8030\n",
      "Epoch: 050, Train: 0.9500, Val: 0.7940, Test: 0.8060\n",
      "Epoch: 051, Train: 0.9500, Val: 0.7980, Test: 0.8100\n",
      "Epoch: 052, Train: 0.9500, Val: 0.7960, Test: 0.8050\n",
      "Epoch: 053, Train: 0.9571, Val: 0.7960, Test: 0.8020\n",
      "Epoch: 054, Train: 0.9571, Val: 0.8000, Test: 0.8030\n",
      "Epoch: 055, Train: 0.9643, Val: 0.8060, Test: 0.8050\n",
      "Epoch: 056, Train: 0.9643, Val: 0.8020, Test: 0.8100\n",
      "Epoch: 057, Train: 0.9643, Val: 0.8000, Test: 0.8110\n",
      "Epoch: 058, Train: 0.9643, Val: 0.8020, Test: 0.8100\n",
      "Epoch: 059, Train: 0.9643, Val: 0.8000, Test: 0.8090\n",
      "Epoch: 060, Train: 0.9571, Val: 0.7960, Test: 0.8100\n",
      "Epoch: 061, Train: 0.9571, Val: 0.7980, Test: 0.8100\n",
      "Epoch: 062, Train: 0.9571, Val: 0.7980, Test: 0.8100\n",
      "Epoch: 063, Train: 0.9571, Val: 0.7980, Test: 0.8130\n",
      "Epoch: 064, Train: 0.9643, Val: 0.7920, Test: 0.8090\n",
      "Epoch: 065, Train: 0.9643, Val: 0.7920, Test: 0.8090\n",
      "Epoch: 066, Train: 0.9643, Val: 0.7920, Test: 0.8100\n",
      "Epoch: 067, Train: 0.9643, Val: 0.7920, Test: 0.8060\n",
      "Epoch: 068, Train: 0.9643, Val: 0.7880, Test: 0.8070\n",
      "Epoch: 069, Train: 0.9714, Val: 0.7860, Test: 0.8120\n",
      "Epoch: 070, Train: 0.9714, Val: 0.7860, Test: 0.8120\n",
      "Epoch: 071, Train: 0.9714, Val: 0.7880, Test: 0.8090\n",
      "Epoch: 072, Train: 0.9714, Val: 0.7900, Test: 0.8080\n",
      "Epoch: 073, Train: 0.9714, Val: 0.7980, Test: 0.8110\n",
      "Epoch: 074, Train: 0.9714, Val: 0.8000, Test: 0.8130\n",
      "Epoch: 075, Train: 0.9786, Val: 0.8020, Test: 0.8160\n",
      "Epoch: 076, Train: 0.9786, Val: 0.7960, Test: 0.8170\n",
      "Epoch: 077, Train: 0.9786, Val: 0.8000, Test: 0.8210\n",
      "Epoch: 078, Train: 0.9786, Val: 0.8020, Test: 0.8200\n",
      "Epoch: 079, Train: 0.9786, Val: 0.8040, Test: 0.8200\n",
      "Epoch: 080, Train: 0.9786, Val: 0.8080, Test: 0.8170\n",
      "Epoch: 081, Train: 0.9786, Val: 0.8080, Test: 0.8150\n",
      "Epoch: 082, Train: 0.9786, Val: 0.8080, Test: 0.8170\n",
      "Epoch: 083, Train: 0.9786, Val: 0.8060, Test: 0.8180\n",
      "Epoch: 084, Train: 0.9786, Val: 0.8060, Test: 0.8200\n",
      "Epoch: 085, Train: 0.9786, Val: 0.8040, Test: 0.8200\n",
      "Epoch: 086, Train: 0.9786, Val: 0.7980, Test: 0.8220\n",
      "Epoch: 087, Train: 0.9857, Val: 0.7960, Test: 0.8260\n",
      "Epoch: 088, Train: 0.9857, Val: 0.8000, Test: 0.8240\n",
      "Epoch: 089, Train: 0.9857, Val: 0.8000, Test: 0.8240\n",
      "Epoch: 090, Train: 0.9857, Val: 0.8040, Test: 0.8250\n",
      "Epoch: 091, Train: 0.9857, Val: 0.8060, Test: 0.8280\n",
      "Epoch: 092, Train: 0.9857, Val: 0.8100, Test: 0.8280\n",
      "Epoch: 093, Train: 0.9857, Val: 0.8120, Test: 0.8300\n",
      "Epoch: 094, Train: 0.9857, Val: 0.8180, Test: 0.8300\n",
      "Epoch: 095, Train: 0.9857, Val: 0.8160, Test: 0.8340\n",
      "Epoch: 096, Train: 0.9857, Val: 0.8160, Test: 0.8360\n",
      "Epoch: 097, Train: 0.9857, Val: 0.8140, Test: 0.8360\n",
      "Epoch: 098, Train: 0.9857, Val: 0.8140, Test: 0.8360\n",
      "Epoch: 099, Train: 0.9857, Val: 0.8080, Test: 0.8370\n",
      "Epoch: 100, Train: 0.9857, Val: 0.8060, Test: 0.8290\n",
      "Epoch: 101, Train: 0.9857, Val: 0.8040, Test: 0.8230\n",
      "Epoch: 102, Train: 0.9857, Val: 0.8080, Test: 0.8220\n",
      "Epoch: 103, Train: 0.9857, Val: 0.8040, Test: 0.8220\n",
      "Epoch: 104, Train: 0.9857, Val: 0.7980, Test: 0.8120\n",
      "Epoch: 105, Train: 0.9857, Val: 0.7980, Test: 0.8120\n",
      "Epoch: 106, Train: 0.9857, Val: 0.8020, Test: 0.8130\n",
      "Epoch: 107, Train: 0.9857, Val: 0.7980, Test: 0.8140\n",
      "Epoch: 108, Train: 0.9857, Val: 0.7940, Test: 0.8130\n",
      "Epoch: 109, Train: 0.9857, Val: 0.7940, Test: 0.8130\n",
      "Epoch: 110, Train: 0.9857, Val: 0.7900, Test: 0.8140\n",
      "Epoch: 111, Train: 0.9857, Val: 0.7920, Test: 0.8160\n",
      "Epoch: 112, Train: 0.9857, Val: 0.7900, Test: 0.8180\n",
      "Epoch: 113, Train: 0.9857, Val: 0.7920, Test: 0.8190\n",
      "Epoch: 114, Train: 0.9857, Val: 0.7920, Test: 0.8200\n",
      "Epoch: 115, Train: 0.9857, Val: 0.7960, Test: 0.8240\n",
      "Epoch: 116, Train: 0.9857, Val: 0.7900, Test: 0.8210\n",
      "Epoch: 117, Train: 0.9857, Val: 0.7940, Test: 0.8210\n",
      "Epoch: 118, Train: 0.9857, Val: 0.7920, Test: 0.8220\n",
      "Epoch: 119, Train: 0.9857, Val: 0.7920, Test: 0.8230\n",
      "Epoch: 120, Train: 0.9857, Val: 0.8000, Test: 0.8250\n",
      "Epoch: 121, Train: 0.9857, Val: 0.8080, Test: 0.8240\n",
      "Epoch: 122, Train: 0.9857, Val: 0.8120, Test: 0.8260\n",
      "Epoch: 123, Train: 0.9857, Val: 0.8120, Test: 0.8270\n",
      "Epoch: 124, Train: 0.9857, Val: 0.8140, Test: 0.8290\n",
      "Epoch: 125, Train: 0.9857, Val: 0.8120, Test: 0.8290\n",
      "Epoch: 126, Train: 0.9857, Val: 0.8180, Test: 0.8310\n",
      "Epoch: 127, Train: 0.9857, Val: 0.8180, Test: 0.8300\n",
      "Epoch: 128, Train: 0.9857, Val: 0.8180, Test: 0.8290\n",
      "Epoch: 129, Train: 0.9857, Val: 0.8160, Test: 0.8300\n",
      "Epoch: 130, Train: 0.9857, Val: 0.8120, Test: 0.8310\n",
      "Epoch: 131, Train: 0.9857, Val: 0.8060, Test: 0.8270\n",
      "Epoch: 132, Train: 0.9857, Val: 0.8060, Test: 0.8260\n",
      "Epoch: 133, Train: 0.9857, Val: 0.8020, Test: 0.8240\n",
      "Epoch: 134, Train: 0.9857, Val: 0.7960, Test: 0.8240\n",
      "Epoch: 135, Train: 0.9857, Val: 0.7960, Test: 0.8220\n",
      "Epoch: 136, Train: 0.9857, Val: 0.8000, Test: 0.8170\n",
      "Epoch: 137, Train: 0.9857, Val: 0.7980, Test: 0.8180\n",
      "Epoch: 138, Train: 0.9857, Val: 0.7960, Test: 0.8140\n",
      "Epoch: 139, Train: 0.9857, Val: 0.7980, Test: 0.8160\n",
      "Epoch: 140, Train: 0.9857, Val: 0.7980, Test: 0.8160\n",
      "Epoch: 141, Train: 0.9857, Val: 0.8000, Test: 0.8220\n",
      "Epoch: 142, Train: 0.9857, Val: 0.8000, Test: 0.8280\n",
      "Epoch: 143, Train: 0.9857, Val: 0.8040, Test: 0.8300\n",
      "Epoch: 144, Train: 0.9857, Val: 0.8040, Test: 0.8300\n",
      "Epoch: 145, Train: 0.9857, Val: 0.8080, Test: 0.8250\n",
      "Epoch: 146, Train: 0.9857, Val: 0.8060, Test: 0.8230\n",
      "Epoch: 147, Train: 0.9857, Val: 0.8040, Test: 0.8220\n",
      "Epoch: 148, Train: 0.9857, Val: 0.8020, Test: 0.8230\n",
      "Epoch: 149, Train: 0.9857, Val: 0.8000, Test: 0.8230\n",
      "Epoch: 150, Train: 0.9857, Val: 0.7960, Test: 0.8250\n",
      "Epoch: 151, Train: 0.9857, Val: 0.7900, Test: 0.8240\n",
      "Epoch: 152, Train: 0.9857, Val: 0.7920, Test: 0.8240\n",
      "Epoch: 153, Train: 0.9857, Val: 0.7920, Test: 0.8220\n",
      "Epoch: 154, Train: 0.9857, Val: 0.7940, Test: 0.8200\n",
      "Epoch: 155, Train: 0.9857, Val: 0.7960, Test: 0.8200\n",
      "Epoch: 156, Train: 0.9857, Val: 0.7980, Test: 0.8200\n",
      "Epoch: 157, Train: 0.9857, Val: 0.8020, Test: 0.8190\n",
      "Epoch: 158, Train: 0.9857, Val: 0.8000, Test: 0.8190\n",
      "Epoch: 159, Train: 0.9857, Val: 0.7980, Test: 0.8210\n",
      "Epoch: 160, Train: 0.9929, Val: 0.8040, Test: 0.8250\n",
      "Epoch: 161, Train: 0.9929, Val: 0.8040, Test: 0.8300\n",
      "Epoch: 162, Train: 0.9929, Val: 0.8060, Test: 0.8290\n",
      "Epoch: 163, Train: 0.9929, Val: 0.8040, Test: 0.8310\n",
      "Epoch: 164, Train: 0.9929, Val: 0.8060, Test: 0.8320\n",
      "Epoch: 165, Train: 0.9929, Val: 0.8080, Test: 0.8300\n",
      "Epoch: 166, Train: 0.9929, Val: 0.8040, Test: 0.8300\n",
      "Epoch: 167, Train: 0.9929, Val: 0.8080, Test: 0.8320\n",
      "Epoch: 168, Train: 0.9929, Val: 0.8120, Test: 0.8320\n",
      "Epoch: 169, Train: 0.9929, Val: 0.8100, Test: 0.8310\n",
      "Epoch: 170, Train: 0.9929, Val: 0.8080, Test: 0.8310\n",
      "Epoch: 171, Train: 0.9929, Val: 0.8080, Test: 0.8300\n",
      "Epoch: 172, Train: 0.9929, Val: 0.8100, Test: 0.8310\n",
      "Epoch: 173, Train: 0.9929, Val: 0.8100, Test: 0.8290\n",
      "Epoch: 174, Train: 0.9929, Val: 0.8100, Test: 0.8300\n",
      "Epoch: 175, Train: 0.9857, Val: 0.8020, Test: 0.8220\n",
      "Epoch: 176, Train: 0.9857, Val: 0.8020, Test: 0.8210\n",
      "Epoch: 177, Train: 0.9857, Val: 0.8000, Test: 0.8190\n",
      "Epoch: 178, Train: 0.9857, Val: 0.8020, Test: 0.8240\n",
      "Epoch: 179, Train: 0.9857, Val: 0.8020, Test: 0.8260\n",
      "Epoch: 180, Train: 0.9929, Val: 0.8060, Test: 0.8250\n",
      "Epoch: 181, Train: 0.9929, Val: 0.8080, Test: 0.8230\n",
      "Epoch: 182, Train: 0.9929, Val: 0.8060, Test: 0.8230\n",
      "Epoch: 183, Train: 0.9929, Val: 0.8080, Test: 0.8230\n",
      "Epoch: 184, Train: 0.9929, Val: 0.8060, Test: 0.8200\n",
      "Epoch: 185, Train: 0.9929, Val: 0.8040, Test: 0.8170\n",
      "Epoch: 186, Train: 0.9929, Val: 0.8060, Test: 0.8140\n",
      "Epoch: 187, Train: 0.9929, Val: 0.8080, Test: 0.8170\n",
      "Epoch: 188, Train: 0.9929, Val: 0.8060, Test: 0.8190\n",
      "Epoch: 189, Train: 0.9929, Val: 0.8040, Test: 0.8190\n",
      "Epoch: 190, Train: 0.9929, Val: 0.8060, Test: 0.8180\n",
      "Epoch: 191, Train: 0.9929, Val: 0.8060, Test: 0.8170\n",
      "Epoch: 192, Train: 0.9929, Val: 0.8040, Test: 0.8200\n",
      "Epoch: 193, Train: 0.9929, Val: 0.8020, Test: 0.8170\n",
      "Epoch: 194, Train: 0.9929, Val: 0.8020, Test: 0.8200\n",
      "Epoch: 195, Train: 0.9929, Val: 0.8040, Test: 0.8200\n",
      "Epoch: 196, Train: 0.9929, Val: 0.8020, Test: 0.8210\n",
      "Epoch: 197, Train: 0.9929, Val: 0.8020, Test: 0.8190\n",
      "Epoch: 198, Train: 0.9929, Val: 0.7980, Test: 0.8210\n",
      "Epoch: 199, Train: 0.9929, Val: 0.8000, Test: 0.8190\n",
      "Epoch: 200, Train: 0.9929, Val: 0.8020, Test: 0.8200\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "dataset = 'Cora'\n",
    "\n",
    "path = osp.join(osp.dirname(osp.realpath('data/cora/raw')), '..', 'data', dataset)\n",
    "dataset = Planetoid(path, dataset, T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GATConv(dataset.num_features, 8, heads=8, dropout=0.6)\n",
    "        # On the Pubmed dataset, use heads=8 in conv2.\n",
    "        self.conv2 = GATConv(\n",
    "            8 * 8, dataset.num_classes, heads=1, concat=True, dropout=0.6)\n",
    "\n",
    "    def forward(self):\n",
    "        # how come there is no need for average before last non-linear layer?? \n",
    "        x = F.dropout(data.x, p=0.6, training=self.training)\n",
    "        x = F.elu(self.conv1(x, data.edge_index))\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, data.edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model, data = Net().to(device), data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    F.nll_loss(model()[data.train_mask], data.y[data.train_mask]).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    logits, accs = model(), []\n",
    "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "        pred = logits[mask].max(1)[1]\n",
    "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        accs.append(acc)\n",
    "    return accs\n",
    "\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    train()\n",
    "    log = 'Epoch: {:03d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "    print(log.format(epoch, *test()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train: 0.1500, Val: 0.1140, Test: 0.1100\n",
      "Epoch: 002, Train: 0.2143, Val: 0.1320, Test: 0.1390\n",
      "Epoch: 003, Train: 0.3143, Val: 0.1860, Test: 0.2000\n",
      "Epoch: 004, Train: 0.3571, Val: 0.2140, Test: 0.2290\n",
      "Epoch: 005, Train: 0.4571, Val: 0.2640, Test: 0.2920\n",
      "Epoch: 006, Train: 0.4714, Val: 0.3080, Test: 0.3320\n",
      "Epoch: 007, Train: 0.5571, Val: 0.3340, Test: 0.3670\n",
      "Epoch: 008, Train: 0.6929, Val: 0.4560, Test: 0.4600\n",
      "Epoch: 009, Train: 0.8071, Val: 0.6520, Test: 0.6800\n",
      "Epoch: 010, Train: 0.8286, Val: 0.6860, Test: 0.7120\n",
      "Epoch: 011, Train: 0.8071, Val: 0.6760, Test: 0.6970\n",
      "Epoch: 012, Train: 0.7714, Val: 0.6580, Test: 0.6750\n",
      "Epoch: 013, Train: 0.7714, Val: 0.6540, Test: 0.6720\n",
      "Epoch: 014, Train: 0.7500, Val: 0.6620, Test: 0.6830\n",
      "Epoch: 015, Train: 0.7500, Val: 0.6820, Test: 0.6910\n",
      "Epoch: 016, Train: 0.8000, Val: 0.7080, Test: 0.7160\n",
      "Epoch: 017, Train: 0.8357, Val: 0.7260, Test: 0.7280\n",
      "Epoch: 018, Train: 0.8429, Val: 0.7260, Test: 0.7370\n",
      "Epoch: 019, Train: 0.8500, Val: 0.7140, Test: 0.7260\n",
      "Epoch: 020, Train: 0.8357, Val: 0.6720, Test: 0.6950\n",
      "Epoch: 021, Train: 0.8286, Val: 0.6460, Test: 0.6680\n",
      "Epoch: 022, Train: 0.8286, Val: 0.6160, Test: 0.6490\n",
      "Epoch: 023, Train: 0.8000, Val: 0.6040, Test: 0.6470\n",
      "Epoch: 024, Train: 0.8143, Val: 0.6020, Test: 0.6320\n",
      "Epoch: 025, Train: 0.8286, Val: 0.6100, Test: 0.6210\n",
      "Epoch: 026, Train: 0.8357, Val: 0.6240, Test: 0.6310\n",
      "Epoch: 027, Train: 0.8429, Val: 0.6180, Test: 0.6100\n",
      "Epoch: 028, Train: 0.8429, Val: 0.6100, Test: 0.6070\n",
      "Epoch: 029, Train: 0.8500, Val: 0.6380, Test: 0.6390\n",
      "Epoch: 030, Train: 0.8714, Val: 0.6500, Test: 0.6430\n",
      "Epoch: 031, Train: 0.8571, Val: 0.6500, Test: 0.6420\n",
      "Epoch: 032, Train: 0.8500, Val: 0.6680, Test: 0.6620\n",
      "Epoch: 033, Train: 0.8571, Val: 0.7020, Test: 0.7030\n",
      "Epoch: 034, Train: 0.8857, Val: 0.7220, Test: 0.7190\n",
      "Epoch: 035, Train: 0.9000, Val: 0.7500, Test: 0.7360\n",
      "Epoch: 036, Train: 0.9000, Val: 0.7740, Test: 0.7520\n",
      "Epoch: 037, Train: 0.9000, Val: 0.7620, Test: 0.7650\n",
      "Epoch: 038, Train: 0.8929, Val: 0.7520, Test: 0.7620\n",
      "Epoch: 039, Train: 0.8714, Val: 0.7360, Test: 0.7390\n",
      "Epoch: 040, Train: 0.8571, Val: 0.7200, Test: 0.7200\n",
      "Epoch: 041, Train: 0.8429, Val: 0.7000, Test: 0.6990\n",
      "Epoch: 042, Train: 0.8429, Val: 0.6540, Test: 0.6740\n",
      "Epoch: 043, Train: 0.8143, Val: 0.6260, Test: 0.6420\n",
      "Epoch: 044, Train: 0.8071, Val: 0.6040, Test: 0.6210\n",
      "Epoch: 045, Train: 0.8000, Val: 0.6020, Test: 0.6140\n",
      "Epoch: 046, Train: 0.8143, Val: 0.6040, Test: 0.6240\n",
      "Epoch: 047, Train: 0.8286, Val: 0.6220, Test: 0.6380\n",
      "Epoch: 048, Train: 0.8500, Val: 0.6420, Test: 0.6590\n",
      "Epoch: 049, Train: 0.8429, Val: 0.6620, Test: 0.6730\n",
      "Epoch: 050, Train: 0.8357, Val: 0.6760, Test: 0.6670\n",
      "Epoch: 051, Train: 0.8357, Val: 0.6700, Test: 0.6570\n",
      "Epoch: 052, Train: 0.8429, Val: 0.6680, Test: 0.6440\n",
      "Epoch: 053, Train: 0.8500, Val: 0.6660, Test: 0.6470\n",
      "Epoch: 054, Train: 0.8500, Val: 0.6500, Test: 0.6240\n",
      "Epoch: 055, Train: 0.8357, Val: 0.6440, Test: 0.6260\n",
      "Epoch: 056, Train: 0.8429, Val: 0.6380, Test: 0.6320\n",
      "Epoch: 057, Train: 0.8214, Val: 0.6400, Test: 0.6310\n",
      "Epoch: 058, Train: 0.8286, Val: 0.6520, Test: 0.6490\n",
      "Epoch: 059, Train: 0.8357, Val: 0.6600, Test: 0.6700\n",
      "Epoch: 060, Train: 0.8500, Val: 0.6740, Test: 0.6770\n",
      "Epoch: 061, Train: 0.8500, Val: 0.6820, Test: 0.6830\n",
      "Epoch: 062, Train: 0.8500, Val: 0.6980, Test: 0.6990\n",
      "Epoch: 063, Train: 0.8571, Val: 0.7140, Test: 0.7100\n",
      "Epoch: 064, Train: 0.8571, Val: 0.7200, Test: 0.7160\n",
      "Epoch: 065, Train: 0.8500, Val: 0.7120, Test: 0.7100\n",
      "Epoch: 066, Train: 0.8571, Val: 0.6980, Test: 0.7060\n",
      "Epoch: 067, Train: 0.8429, Val: 0.6980, Test: 0.7040\n",
      "Epoch: 068, Train: 0.8571, Val: 0.6920, Test: 0.6980\n",
      "Epoch: 069, Train: 0.8714, Val: 0.6940, Test: 0.6990\n",
      "Epoch: 070, Train: 0.8571, Val: 0.6960, Test: 0.6870\n",
      "Epoch: 071, Train: 0.8429, Val: 0.6640, Test: 0.6630\n",
      "Epoch: 072, Train: 0.8286, Val: 0.6260, Test: 0.6340\n",
      "Epoch: 073, Train: 0.7929, Val: 0.5980, Test: 0.6060\n",
      "Epoch: 074, Train: 0.7929, Val: 0.5800, Test: 0.5960\n",
      "Epoch: 075, Train: 0.7857, Val: 0.5800, Test: 0.5920\n",
      "Epoch: 076, Train: 0.7786, Val: 0.5740, Test: 0.5910\n",
      "Epoch: 077, Train: 0.7786, Val: 0.5740, Test: 0.5820\n",
      "Epoch: 078, Train: 0.7786, Val: 0.5820, Test: 0.5850\n",
      "Epoch: 079, Train: 0.7714, Val: 0.5720, Test: 0.5740\n",
      "Epoch: 080, Train: 0.7357, Val: 0.5620, Test: 0.5580\n",
      "Epoch: 081, Train: 0.7500, Val: 0.5500, Test: 0.5360\n",
      "Epoch: 082, Train: 0.7500, Val: 0.5580, Test: 0.5340\n",
      "Epoch: 083, Train: 0.7429, Val: 0.5520, Test: 0.5360\n",
      "Epoch: 084, Train: 0.7429, Val: 0.5580, Test: 0.5390\n",
      "Epoch: 085, Train: 0.7429, Val: 0.5600, Test: 0.5380\n",
      "Epoch: 086, Train: 0.7429, Val: 0.5580, Test: 0.5400\n",
      "Epoch: 087, Train: 0.7286, Val: 0.5540, Test: 0.5520\n",
      "Epoch: 088, Train: 0.7357, Val: 0.5520, Test: 0.5450\n",
      "Epoch: 089, Train: 0.7500, Val: 0.5500, Test: 0.5410\n",
      "Epoch: 090, Train: 0.7429, Val: 0.5580, Test: 0.5430\n",
      "Epoch: 091, Train: 0.7571, Val: 0.5760, Test: 0.5470\n",
      "Epoch: 092, Train: 0.7643, Val: 0.5820, Test: 0.5540\n",
      "Epoch: 093, Train: 0.7643, Val: 0.5840, Test: 0.5670\n",
      "Epoch: 094, Train: 0.7571, Val: 0.5980, Test: 0.5850\n",
      "Epoch: 095, Train: 0.7786, Val: 0.6080, Test: 0.5960\n",
      "Epoch: 096, Train: 0.8000, Val: 0.6220, Test: 0.6060\n",
      "Epoch: 097, Train: 0.8071, Val: 0.6260, Test: 0.6210\n",
      "Epoch: 098, Train: 0.8286, Val: 0.6320, Test: 0.6160\n",
      "Epoch: 099, Train: 0.8357, Val: 0.6320, Test: 0.6160\n",
      "Epoch: 100, Train: 0.8357, Val: 0.6260, Test: 0.6150\n",
      "Epoch: 101, Train: 0.8286, Val: 0.6260, Test: 0.6150\n",
      "Epoch: 102, Train: 0.8214, Val: 0.6120, Test: 0.6020\n",
      "Epoch: 103, Train: 0.8000, Val: 0.5940, Test: 0.5890\n",
      "Epoch: 104, Train: 0.7786, Val: 0.5660, Test: 0.5740\n",
      "Epoch: 105, Train: 0.7643, Val: 0.5480, Test: 0.5570\n",
      "Epoch: 106, Train: 0.7643, Val: 0.5340, Test: 0.5490\n",
      "Epoch: 107, Train: 0.7714, Val: 0.5320, Test: 0.5410\n",
      "Epoch: 108, Train: 0.7714, Val: 0.5340, Test: 0.5340\n",
      "Epoch: 109, Train: 0.7643, Val: 0.5340, Test: 0.5350\n",
      "Epoch: 110, Train: 0.7786, Val: 0.5320, Test: 0.5350\n",
      "Epoch: 111, Train: 0.7857, Val: 0.5460, Test: 0.5370\n",
      "Epoch: 112, Train: 0.7714, Val: 0.5440, Test: 0.5360\n",
      "Epoch: 113, Train: 0.7714, Val: 0.5480, Test: 0.5390\n",
      "Epoch: 114, Train: 0.7714, Val: 0.5540, Test: 0.5410\n",
      "Epoch: 115, Train: 0.7714, Val: 0.5540, Test: 0.5450\n",
      "Epoch: 116, Train: 0.7786, Val: 0.5620, Test: 0.5500\n",
      "Epoch: 117, Train: 0.7929, Val: 0.5720, Test: 0.5520\n",
      "Epoch: 118, Train: 0.7929, Val: 0.5720, Test: 0.5540\n",
      "Epoch: 119, Train: 0.7857, Val: 0.5740, Test: 0.5570\n",
      "Epoch: 120, Train: 0.7714, Val: 0.5780, Test: 0.5580\n",
      "Epoch: 121, Train: 0.8071, Val: 0.5880, Test: 0.5700\n",
      "Epoch: 122, Train: 0.8000, Val: 0.6040, Test: 0.5830\n",
      "Epoch: 123, Train: 0.8000, Val: 0.6060, Test: 0.5880\n",
      "Epoch: 124, Train: 0.8071, Val: 0.6160, Test: 0.5980\n",
      "Epoch: 125, Train: 0.8357, Val: 0.6280, Test: 0.6150\n",
      "Epoch: 126, Train: 0.8357, Val: 0.6340, Test: 0.6260\n",
      "Epoch: 127, Train: 0.8357, Val: 0.6320, Test: 0.6220\n",
      "Epoch: 128, Train: 0.8357, Val: 0.6280, Test: 0.6230\n",
      "Epoch: 129, Train: 0.8357, Val: 0.6260, Test: 0.6220\n",
      "Epoch: 130, Train: 0.8357, Val: 0.6280, Test: 0.6220\n",
      "Epoch: 131, Train: 0.8286, Val: 0.6180, Test: 0.6130\n",
      "Epoch: 132, Train: 0.8143, Val: 0.6120, Test: 0.6070\n",
      "Epoch: 133, Train: 0.8071, Val: 0.5960, Test: 0.5940\n",
      "Epoch: 134, Train: 0.8000, Val: 0.5800, Test: 0.5730\n",
      "Epoch: 135, Train: 0.7571, Val: 0.5740, Test: 0.5600\n",
      "Epoch: 136, Train: 0.7357, Val: 0.5560, Test: 0.5430\n",
      "Epoch: 137, Train: 0.7286, Val: 0.5380, Test: 0.5250\n",
      "Epoch: 138, Train: 0.7286, Val: 0.5340, Test: 0.5160\n",
      "Epoch: 139, Train: 0.7357, Val: 0.5260, Test: 0.5090\n",
      "Epoch: 140, Train: 0.7429, Val: 0.5220, Test: 0.5120\n",
      "Epoch: 141, Train: 0.7500, Val: 0.5320, Test: 0.5160\n",
      "Epoch: 142, Train: 0.7714, Val: 0.5380, Test: 0.5180\n",
      "Epoch: 143, Train: 0.7571, Val: 0.5340, Test: 0.5190\n",
      "Epoch: 144, Train: 0.7500, Val: 0.5340, Test: 0.5180\n",
      "Epoch: 145, Train: 0.7500, Val: 0.5320, Test: 0.5170\n",
      "Epoch: 146, Train: 0.7500, Val: 0.5260, Test: 0.5110\n",
      "Epoch: 147, Train: 0.7429, Val: 0.5140, Test: 0.5070\n",
      "Epoch: 148, Train: 0.7429, Val: 0.5120, Test: 0.5070\n",
      "Epoch: 149, Train: 0.7357, Val: 0.5140, Test: 0.5080\n",
      "Epoch: 150, Train: 0.7357, Val: 0.5160, Test: 0.5030\n",
      "Epoch: 151, Train: 0.7429, Val: 0.5100, Test: 0.5010\n",
      "Epoch: 152, Train: 0.7429, Val: 0.5080, Test: 0.5010\n",
      "Epoch: 153, Train: 0.7429, Val: 0.5080, Test: 0.5040\n",
      "Epoch: 154, Train: 0.7429, Val: 0.5160, Test: 0.5100\n",
      "Epoch: 155, Train: 0.7500, Val: 0.5220, Test: 0.5130\n",
      "Epoch: 156, Train: 0.7643, Val: 0.5240, Test: 0.5130\n",
      "Epoch: 157, Train: 0.7643, Val: 0.5260, Test: 0.5160\n",
      "Epoch: 158, Train: 0.7714, Val: 0.5300, Test: 0.5220\n",
      "Epoch: 159, Train: 0.7786, Val: 0.5320, Test: 0.5280\n",
      "Epoch: 160, Train: 0.7857, Val: 0.5400, Test: 0.5360\n",
      "Epoch: 161, Train: 0.7857, Val: 0.5500, Test: 0.5450\n",
      "Epoch: 162, Train: 0.7929, Val: 0.5520, Test: 0.5510\n",
      "Epoch: 163, Train: 0.8071, Val: 0.5500, Test: 0.5560\n",
      "Epoch: 164, Train: 0.8071, Val: 0.5600, Test: 0.5690\n",
      "Epoch: 165, Train: 0.8071, Val: 0.5660, Test: 0.5770\n",
      "Epoch: 166, Train: 0.8071, Val: 0.5720, Test: 0.5820\n",
      "Epoch: 167, Train: 0.8071, Val: 0.5840, Test: 0.5830\n",
      "Epoch: 168, Train: 0.8143, Val: 0.5960, Test: 0.5970\n",
      "Epoch: 169, Train: 0.8143, Val: 0.5960, Test: 0.5990\n",
      "Epoch: 170, Train: 0.8214, Val: 0.6060, Test: 0.6030\n",
      "Epoch: 171, Train: 0.8286, Val: 0.6100, Test: 0.6070\n",
      "Epoch: 172, Train: 0.8286, Val: 0.6040, Test: 0.6030\n",
      "Epoch: 173, Train: 0.8286, Val: 0.5960, Test: 0.6030\n",
      "Epoch: 174, Train: 0.8214, Val: 0.5960, Test: 0.5990\n",
      "Epoch: 175, Train: 0.8286, Val: 0.5900, Test: 0.5990\n",
      "Epoch: 176, Train: 0.8214, Val: 0.5900, Test: 0.5970\n",
      "Epoch: 177, Train: 0.8214, Val: 0.6000, Test: 0.5990\n",
      "Epoch: 178, Train: 0.8214, Val: 0.6060, Test: 0.6050\n",
      "Epoch: 179, Train: 0.8286, Val: 0.6200, Test: 0.6140\n",
      "Epoch: 180, Train: 0.8500, Val: 0.6180, Test: 0.6110\n",
      "Epoch: 181, Train: 0.8429, Val: 0.6140, Test: 0.6080\n",
      "Epoch: 182, Train: 0.8357, Val: 0.6080, Test: 0.5980\n",
      "Epoch: 183, Train: 0.8429, Val: 0.5960, Test: 0.5930\n",
      "Epoch: 184, Train: 0.8286, Val: 0.5920, Test: 0.5890\n",
      "Epoch: 185, Train: 0.8143, Val: 0.5780, Test: 0.5750\n",
      "Epoch: 186, Train: 0.8071, Val: 0.5680, Test: 0.5600\n",
      "Epoch: 187, Train: 0.8071, Val: 0.5620, Test: 0.5540\n",
      "Epoch: 188, Train: 0.8071, Val: 0.5560, Test: 0.5490\n",
      "Epoch: 189, Train: 0.8071, Val: 0.5500, Test: 0.5480\n",
      "Epoch: 190, Train: 0.8071, Val: 0.5500, Test: 0.5500\n",
      "Epoch: 191, Train: 0.8000, Val: 0.5460, Test: 0.5440\n",
      "Epoch: 192, Train: 0.8000, Val: 0.5460, Test: 0.5490\n",
      "Epoch: 193, Train: 0.8071, Val: 0.5480, Test: 0.5590\n",
      "Epoch: 194, Train: 0.8143, Val: 0.5600, Test: 0.5690\n",
      "Epoch: 195, Train: 0.8143, Val: 0.5800, Test: 0.5780\n",
      "Epoch: 196, Train: 0.8143, Val: 0.5820, Test: 0.5900\n",
      "Epoch: 197, Train: 0.8143, Val: 0.5900, Test: 0.6030\n",
      "Epoch: 198, Train: 0.8143, Val: 0.6000, Test: 0.6020\n",
      "Epoch: 199, Train: 0.8143, Val: 0.5980, Test: 0.6070\n",
      "Epoch: 200, Train: 0.8143, Val: 0.5960, Test: 0.6100\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "dataset = 'Cora'\n",
    "\n",
    "path = osp.join(osp.dirname(osp.realpath('data/data/Cora')), '..', 'data', dataset)\n",
    "dataset = Planetoid(path, dataset, T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GATConv(dataset.num_features, 8, heads=8, dropout=0.6)\n",
    "        # On the Pubmed dataset, use heads=8 in conv2.\n",
    "        self.conv2 = GATConv(\n",
    "            8 * 8, dataset.num_classes, heads=1, concat=True, dropout=0.6)\n",
    "\n",
    "    def forward(self):\n",
    "        # how come there is no need for average before last non-linear layer?? \n",
    "        x = F.dropout(data.x, p=0.6, training=self.training)\n",
    "        x = F.elu(self.conv1(x, data.edge_index))\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = 1/8 * self.conv2(x, data.edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model, data = Net().to(device), data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    F.nll_loss(model()[data.train_mask], data.y[data.train_mask]).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    logits, accs = model(), []\n",
    "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "        pred = logits[mask].max(1)[1]\n",
    "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        accs.append(acc)\n",
    "    return accs\n",
    "\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    train()\n",
    "    log = 'Epoch: {:03d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "    print(log.format(epoch, *test()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = [1,2,3,4]\n",
    "y = [1,3,4,5]\n",
    "label = [0,1,1,0]\n",
    "# plt.scatter(x,y,label=label)\n",
    "plt.scatter(x,y)\n",
    "plt.legend(label)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = [4,8,12,16,1,4,9,16]\n",
    "y = [1,4,9,16,4,8,12,3]\n",
    "label = [0,1,2,3,0,1,2,3]\n",
    "colors = ['red','green','blue','purple']\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "# plt.cm.get_cmap('hsv', 10, )\n",
    "plt.scatter(x, y,  c=label, cmap=matplotlib.colors.ListedColormap())\n",
    "# plt.scatter(x, y, c=label, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "\n",
    "cb = plt.colorbar()\n",
    "loc = np.arange(0,max(label),max(label)/float(len(colors)))\n",
    "cb.set_ticks(loc)\n",
    "cb.set_ticklabels(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWwAAABzCAYAAAC8aDhDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAGuElEQVR4nO3c24tddxnG8e9jkmqhxYopGJpqFERQ8ZCEEClIUS+KSnthL3rhoYI3iqh4IeqFon+AiHpRPBTquVJFYmkRpYpXRpOYaEuMRBEMVmIrTSueSH292KvtsJ3JrMnszNpv+H5gw56s39p55s38nj17zZ6kqpAkLb9nTR1AkjSOhS1JTVjYktSEhS1JTVjYktSEhS1JTWzfzMk7k9qzoCCCY/umTnD5qOMOc5H2PXli6giXlaOcf6Sqrt3oednM+7D3J3Xkos/WvCt9S/zC/Ot5DnOR6rFdU0e4rIS/HK2q/Rs9z0siktSEhS1JTVjYktSEhS1JTVjYktSEhS1JTVjYktSEhS1JTVjYktSEhS1JTVjYktSEhS1JTVjYktSEhS1JTVjYktSEhS1JTVjYktSEhS1JTVjYktSEhS1JTVjYktSEhS1JTVjYktSEhS1JTVjYktSEhS1JTVjYktSEhS1JTVjYktSEhS1JTVjYktSEhS1JTVjYktSEhS1JTVjYktSEhS1JTVjYktSEhS1JTVjYktSEhS1JTVjYktSEhS1JTVjYktSEhS1JTVjYktSEhS1JTVjYktSEhS1JTVjYktSEhS1JTVjYktSEhS1JTVjYktSEhS1JTVjYktSEhS1JTVjYktSEhS1JTVjYktSEhS1JTVjYktSEhS1JTVjYktSEhS1JTVjYktSEhS1JTVjYktSEhS1JTVjYktSEhS1JTVjYktSEhS1JTVjYktSEhS1JTVjYktSEhS1JTVjYktSEhS1JTVjYktSEhS1JTVjYktSEhS1JTaSqLv7k5Ang1OLiXDI7gUemDjGCORenQ0Yw56J1yfmyqrp6oydt3+Rfeqqq9m/yMS65JEfMuTgdcnbICOZctE45L+Y8L4lIUhMWtiQ1sdnC/uJCUlx65lysDjk7ZARzLtplnXNTP3SUJG0dL4lIUhOjCjvJTUlOJTmd5KOrHH92kruH44eT7Fl00DFG5Lw9yV+THB9u75kg451JziZ5cI3jSfK54XP4dZK9W51xyLFezhuTnFsxy09MkPH6JD9JcjLJQ0k+uMqayec5MucyzPM5SX6R5MSQ81OrrJl8r4/MOfleH3JsS/KrJPeucmzjs6yqC96AbcDvgZcAVwAngJfPrXkfcMdw/zbg7vUed9G3kTlvB76w1dnmMrwe2As8uMbxNwP3AwEOAoeXNOeNwL0Tz3IXsHe4fzXwu1X+zSef58icyzDPAFcN93cAh4GDc2uWYa+PyTn5Xh9yfBj45mr/thczyzHfYR8ATlfVH6rqP8C3gVvm1twC3DXcvwd4Y5KMeOxFGpNzclX1M+BvF1hyC/DVmvk5cE2SXVuT7hkjck6uqh6uqmPD/SeAk8B1c8smn+fInJMbZvT34cMdw23+h1yT7/WROSeXZDfwFuDLayzZ8CzHFPZ1wJ9WfHyG//9ie3pNVZ0HzgHPH/HYizQmJ8DbhpfG9yS5fmuibcjYz2MZvG54WXp/kldMGWR4OflaZt9trbRU87xATliCeQ4v4Y8DZ4EfVdWa85xwr4/JCdPv9c8CHwH+u8bxDc9yTGGv1vjzz2Zj1lxqYzL8ANhTVa8Cfswzz27LZBlmOcYx4EVV9Wrg88D3pwqS5Crgu8CHqurx+cOrnDLJPNfJuRTzrKonq+o1wG7gQJJXzi1ZinmOyDnpXk/yVuBsVR290LJV/uyCsxxT2GeAlc9Ou4E/r7UmyXbguWz9y+l1c1bVo1X17+HDLwH7tijbRoyZ9+Sq6vGnXpZW1X3AjiQ7tzpHkh3MSvAbVfW9VZYsxTzXy7ks81yR5zHgp8BNc4eWYa8/ba2cS7DXbwBuTvJHZpdn35Dk63NrNjzLMYX9S+ClSV6c5ApmF8cPza05BLxruH8r8EANV9K30Lo5565d3szsWuKyOQS8c3h3w0HgXFU9PHWoeUle8NT1tiQHmH0tPbrFGQJ8BThZVZ9ZY9nk8xyTc0nmeW2Sa4b7VwJvAn47t2zyvT4m59R7vao+VlW7q2oPsy56oKrePrdsw7Nc9z9/qqrzSd4P/JDZOzHurKqHknwaOFJVh5h9MX4tyWlmzxC3beBzW4iROT+Q5Gbg/JDz9q3OmeRbzN4RsDPJGeCTzH5oQlXdAdzH7J0Np4F/AO/e6owjc94KvDfJeeCfwG0TPEnfALwD+M1wPRPg48ALV+RchnmOybkM89wF3JVkG7MnjO9U1b3LttdH5px8r69ms7P0Nx0lqQl/01GSmrCwJakJC1uSmrCwJakJC1uSmrCwJakJC1uSmrCwJamJ/wHITeUII4MD2gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_cmap(n, name='hsv'):\n",
    "    '''Returns a function that maps each index in 0, 1, ..., n-1 to a distinct \n",
    "    RGB color; the keyword argument name must be a standard mpl colormap name.'''\n",
    "    return plt.cm.get_cmap(name, n)\n",
    "\n",
    "def main():\n",
    "    N = 4\n",
    "    fig=plt.figure()\n",
    "    ax=fig.add_subplot(111)   \n",
    "    plt.axis('scaled')\n",
    "    ax.set_xlim([ 0, N])\n",
    "    ax.set_ylim([-0.5, 0.5])\n",
    "    cmap = get_cmap(N)\n",
    "    for i in range(N):\n",
    "        rect = plt.Rectangle((i, -0.5), 1, 1, facecolor=cmap(i))\n",
    "        ax.add_artist(rect)\n",
    "    ax.set_yticks([])\n",
    "    plt.show()\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elif\n"
     ]
    }
   ],
   "source": [
    "if False:\n",
    "    print(\"if\")\n",
    "elif True:\n",
    "    print(\"elif\")\n",
    "else:\n",
    "    print(\"else\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6258, -0.9976, -1.0214, -0.3826,  0.9424],\n",
      "        [ 0.4172,  0.8993, -0.7114,  1.5132, -0.4366],\n",
      "        [-1.1204,  0.5191,  1.2481,  0.6030, -0.2336]], requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\awannaphasch2016\\AppData\\Local\\Continuum\\anaconda3\\envs\\pytorch_python3.7\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.1325, -2.5043, -2.5281, -1.8893, -0.5643],\n",
      "        [-1.8501, -1.3681, -2.9788, -0.7542, -2.7040],\n",
      "        [-3.2134, -1.5740, -0.8450, -1.4900, -2.3266]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor(-66.4673, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "# input = torch.randn(3, 5, requires_grad=True)\n",
    "x = np.array([[100000,  100,  1.9125, -0.8701, -1.4935],\n",
    "        [100,  100000, -1.9865,  1.9137,  0.5035],\n",
    "        [ 1.1986, -1.1636, 100000, 10, -0.5981]])\n",
    "x = torch.tensor(x)\n",
    "# each element in target has to have 0 <= value < C\n",
    "target = torch.tensor([1, 0, 4])\n",
    "# output = F.nll_loss(F.log_softmax(input), target)\n",
    "output = F.nll_loss(x, target)\n",
    "# output.backward()\n",
    "print(input)\n",
    "print(F.log_softmax(input))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0163,  0.2736, -0.7971, -0.8804, -1.9788],\n",
      "        [-1.1030,  0.2063, -1.6021,  1.8479, -1.1519],\n",
      "        [ 1.4482,  0.9897, -0.5516, -0.9631,  2.0321]], requires_grad=True)\n",
      "tensor(-100., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "x = np.array([[0,  100,  100000, -0.8701, -1.4935],\n",
    "        [100,  10000, -1.9865,  1.9137,  0.5035],\n",
    "        [ 1.1986, -1.1636, 0, 10000, 100]])\n",
    "x = torch.tensor(x)\n",
    "# each element in target has to have 0 <= value < C\n",
    "target = torch.tensor([1, 0, 4])\n",
    "# output = F.nll_loss(F.log_softmax(input), target)\n",
    "output = F.nll_loss(x, target)\n",
    "# output.backward()\n",
    "print(input)\n",
    "# print(F.log_softmax(input))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-100., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "# input = torch.randn(3, 5, requires_grad=True)\n",
    "x = np.array([[0,  100,  0, -0.8701, -1.4935],\n",
    "        [100,  0, -1.9865,  1.9137,  0.5035],\n",
    "        [ 1.1986, -1.1636, 0, 0, 100]])\n",
    "x = torch.tensor(x)\n",
    "# each element in target has to have 0 <= value < C\n",
    "target = torch.tensor([1, 0, 4])\n",
    "# output = F.nll_loss(F.log_softmax(input), target)\n",
    "output = F.nll_loss(x, target)\n",
    "# output.backward()\n",
    "# print(input)\n",
    "# print(F.log_softmax(input))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-100., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "# x = torch.randn(3, 5, requires_grad=True)\n",
    "x = np.array([[0,  100,  0, 0.5, 5],\n",
    "        [100,  0, -1.4,  13,  0.545],\n",
    "        [ 1.1986, 0, 0, 0, 100]])\n",
    "x = torch.tensor(x)\n",
    "# each element in target has to have 0 <= value < C\n",
    "target = torch.tensor([1, 0, 4])\n",
    "# output = F.nll_loss(F.log_softmax(input), target)\n",
    "output = F.nll_loss(x, target)\n",
    "# output.backward()\n",
    "# print(input)\n",
    "# print(F.log_softmax(input))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features=None, categories=None, drop=None,\n",
       "              dtype=<class 'numpy.float64'>, handle_unknown='ignore',\n",
       "              n_values=None, sparse=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "X = [['Male', 1], ['Female', 3], ['Female', 2]]\n",
    "enc.fit(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.transform([['Female', 1], ['Male', 5], ['no',3]]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Male', 1],\n",
       "       [None, None]], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.inverse_transform([[0, 1, 1, 0, 0], [0, 0, 0, 0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 999), (3, 3), (6, 777), (4, 666)]\n",
      "[[0 1 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx \n",
    "edges = [(2,999),(3,3), (6,777), (4,666)]\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(edges)\n",
    "# print(G.edges)\n",
    "print(G.edges)\n",
    "print(nx.adjacency_matrix(G).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  0  7 10  0]\n",
      " [ 1  0  9 11  0  9]\n",
      " [ 0  9  0  0 12  7]\n",
      " [ 7 11  0  0  8 14]\n",
      " [10  0 12  8  0  8]\n",
      " [ 0  9  7 14  8  0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "def weighted_adjmatrix(adjlist, nodes):\n",
    "    '''Returns a (weighted) adjacency matrix as a NumPy array.'''\n",
    "    matrix = []\n",
    "    for node in nodes:\n",
    "        weights = {endnode:int(weight)\n",
    "                   for w in adjlist.get(node, {})\n",
    "                   for endnode, weight in w.items()}\n",
    "        matrix.append([weights.get(endnode, 0) for endnode in nodes])\n",
    "    matrix = numpy.array(matrix)\n",
    "    return matrix + matrix.transpose()\n",
    "\n",
    "graph = {'1': [{'2':'1'}, {'4':'7'}, {'5':'10'}],\n",
    "    '2': [{'3':'9'}, {'4':'11'}, {'6':'9'}],\n",
    "    '3': [{'5':'12'}, {'6':'7'}],\n",
    "    '4': [{'5':'8'}, {'6':'14'}],\n",
    "    '5': [{'6':'8'}]}\n",
    "x = weighted_adjmatrix(graph, nodes=list('123456'))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2.]\n",
      "[[0. 1. 2.]\n",
      " [3. 4. 5.]\n",
      " [6. 7. 8.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x1 = np.arange(9.0).reshape((3, 3))\n",
    "x2 = np.arange(3.0)\n",
    "# np.add(x1,x2)\n",
    "print(x2)\n",
    "print(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "x = 0\n",
    "y = np.arange(3)\n",
    "x = np.add(x,y)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse matrix with scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse as sp\n",
    "from scipy.stats import uniform\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_ind = np.array([0,1,1,3,4])\n",
    "col_ind = np.array([0,2,4,3,4])\n",
    "data = np.array([1,2,3,4,5], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1.0\n",
      "  (1, 2)\t2.0\n",
      "  (1, 4)\t3.0\n",
      "  (3, 3)\t4.0\n",
      "  (4, 4)\t5.0\n"
     ]
    }
   ],
   "source": [
    "row_ind = np.array([0,1,1,3,4])\n",
    "col_ind = np.array([0,2,4,3,4])\n",
    "data = np.array([1,2,3,4,5], dtype=float)\n",
    "mat_coo = sp.coo_matrix((data, (row_ind, col_ind)))\n",
    "print(mat_coo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.74908024, 1.90142861, 1.46398788, 1.19731697],\n",
       "       [0.31203728, 0.31198904, 0.11616722, 1.73235229],\n",
       "       [1.20223002, 1.41614516, 0.04116899, 1.9398197 ],\n",
       "       [1.66488528, 0.42467822, 0.36364993, 0.36680902]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(seed=42)\n",
    "data = uniform.rvs(size=16, loc=0, scale=2)\n",
    "data = np.reshape(data, (4,4))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.90142861, 1.46398788, 1.19731697],\n",
       "       [0.        , 0.        , 0.        , 1.73235229],\n",
       "       [1.20223002, 1.41614516, 0.        , 1.9398197 ],\n",
       "       [1.66488528, 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data < 1] = 0\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csr = sp.csr_matrix(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "data_size = data.nbytes\n",
    "print(data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "data_csr = sp.csr_matrix(data)\n",
    "data_csr_size = data_csr.data.size/(1024**2)\n",
    "print(data_csr_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1.0\n",
      "  (1, 2)\t2.0\n",
      "  (1, 4)\t3.0\n",
      "  (3, 3)\t4.0\n",
      "  (4, 4)\t5.0\n"
     ]
    }
   ],
   "source": [
    "row_ind = np.array([0,1,1,3,4])\n",
    "col_ind = np.array([0,2,4,3,4])\n",
    "data = np.array([1,2,3,4,5], dtype=float)\n",
    "mat_coo = sp.coo_matrix((data, (row_ind, col_ind)))\n",
    "print(mat_coo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1.0\n",
      "  (1, 0)\t3.0\n",
      "  (3, 0)\t4.0\n",
      "  (4, 0)\t5.0\n",
      "===\n",
      "  (0, 0)\t1.0\n",
      "  (0, 2)\t2.0\n",
      "  (0, 3)\t4.0\n",
      "  (0, 4)\t5.0\n"
     ]
    }
   ],
   "source": [
    "x = mat_coo.max(axis=1) # max value for each rows\n",
    "y = mat_coo.max(axis=0) # max value for each cols\n",
    "print(x)\n",
    "print(\"===\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1., 0., 0., 0., 0.],\n",
       "        [0., 0., 2., 0., 3.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 4., 0.],\n",
       "        [0., 0., 0., 0., 5.]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_coo.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1.],\n",
       "        [3.],\n",
       "        [0.],\n",
       "        [4.],\n",
       "        [5.]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1., 0., 2., 4., 5.]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 2. 0. 3. 0. 0. 2. 0. 3.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 4. 0. 0. 0. 0. 4. 0.]\n",
      " [0. 0. 0. 0. 5. 0. 0. 0. 0. 5.]]\n",
      "===\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 0. 2. 0. 3.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 4. 0.]\n",
      " [0. 0. 0. 0. 5.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 2. 0. 3.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 4. 0.]\n",
      " [0. 0. 0. 0. 5.]]\n",
      "===\n",
      "[[1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 2. 0. 0. 0. 0. 2. 0. 0. 0.]\n",
      " [0. 0. 0. 4. 0. 0. 0. 0. 4. 0.]\n",
      " [0. 3. 0. 0. 5. 0. 3. 0. 0. 5.]]\n"
     ]
    }
   ],
   "source": [
    "x = sp.hstack([mat_coo, mat_coo])\n",
    "print(x.todense())\n",
    "x = sp.vstack([mat_coo, mat_coo])\n",
    "print(\"===\")\n",
    "print(x.todense())\n",
    "print('===')\n",
    "print(x.transpose().todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (2, 2)\t1.0\n",
      "  (3, 3)\t1.0\n",
      "  (4, 4)\t1.0\n"
     ]
    }
   ],
   "source": [
    "identity_mx = sp.eye(5).tocoo()\n",
    "print(identity_mx.tocoo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n",
      "[0 1 2 3 4]\n",
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(identity_mx.row)\n",
    "print(identity_mx.col)\n",
    "print(identity_mx.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "row, column, and data array must all be the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-624c756d5681>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcol_ind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmat_coo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoo_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrow_ind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_ind\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmat_coo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\pytorch_python3.7\\lib\\site-packages\\scipy\\sparse\\coo.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m    196\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\pytorch_python3.7\\lib\\site-packages\\scipy\\sparse\\coo.py\u001b[0m in \u001b[0;36m_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_native\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnnz\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'row index exceeds matrix dimensions'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\pytorch_python3.7\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36mnnz\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    248\u001b[0m         \u001b[0mcount_nonzero\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mNumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mnon\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mzero\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \"\"\"\n\u001b[1;32m--> 250\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgetformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\pytorch_python3.7\\lib\\site-packages\\scipy\\sparse\\coo.py\u001b[0m in \u001b[0;36mgetnnz\u001b[1;34m(self, axis)\u001b[0m\n\u001b[0;32m    243\u001b[0m             \u001b[0mnnz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnnz\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mnnz\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m                 raise ValueError('row, column, and data array must all be the '\n\u001b[0m\u001b[0;32m    246\u001b[0m                                  'same length')\n\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: row, column, and data array must all be the same length"
     ]
    }
   ],
   "source": [
    "row_ind = np.array([0,1,1,3,4])\n",
    "col_ind = np.array([0,2,4,3,4])\n",
    "data = np.array([1,2,3,4], dtype=float)\n",
    "mat_coo = sp.coo_matrix((data, (row_ind, col_ind)))\n",
    "print(mat_coo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(3 * 3 ).reshape(3,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.identity(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708, 1433])\n",
      "Accuracy: 0.8020\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_features, 16)\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        print(x.shape)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net().to(device)\n",
    "data = dataset[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "_, pred = model(data).max(dim=1)\n",
    "correct = float (pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "acc = correct / data.test_mask.sum().item()\n",
    "print('Accuracy: {:.4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'one': 1}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DoppelDict(dict):\n",
    "    def __setitem__(self, key,value):\n",
    "            super().__setitem__(key, [value]*2)\n",
    "dd =DoppelDict(one=1)\n",
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd['two'] =2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'one': 1, 'two': [2, 2]}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.update(three=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'one': 1, 'two': [2, 2], 'three': 3}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AnswerDict(dict):\n",
    "    def __getitem__(self, key):\n",
    "        return 42\n",
    "    \n",
    "ad = AnswerDict(a='foo')\n",
    "ad['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.update(ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 'foo'}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'one': [1, 1]}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections \n",
    "class DoppelDict2(collections.UserDict):\n",
    "    def __setitem__(self, key, value):\n",
    "        super().__setitem__(key, [value] *2)\n",
    "dd = DoppelDict2(one=1)\n",
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd['two'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__init__, get, update dro mdict refused to copperate \n",
    "with overriden __missing__, c__contains__ , and _-setitem__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrKeyDict(dict):\n",
    "    def __init__(self, iteable=None, **kwds):\n",
    "        super().__init__()\n",
    "        self.update(iteable, **kwds)\n",
    "    def __missing__(self):\n",
    "        if isinstance(key, str):\n",
    "            raise KeyError(key)\n",
    "        return self[str(key)]\n",
    "    def __contains(self,key):\n",
    "        return key in self.keys() or str(key) in self.keys()\n",
    "    def __setitem__(self,key,item):\n",
    "        super().__setitem__(str(key),item)\n",
    "    def get(self, key, default=None):\n",
    "        try:\n",
    "            return self[key]\n",
    "        except:\n",
    "            return default # try using raise KeyError\n",
    "    def update(self, iterable=None, **kwds):\n",
    "        if iterable is not None:\n",
    "            if isinstnace(iterable, collections.abc.Mapping):\n",
    "                pairs= iterable.items()\n",
    "            else:\n",
    "                pairs((k,v) for k,v in iterable)\n",
    "            for key, value in pairs:\n",
    "                self[key] = value\n",
    "        if kwds:\n",
    "            self.update(kwds)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14205973 0.76664038 0.0282433  0.06305659]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=4,\n",
    "                           n_informative=2, n_redundant=0,\n",
    "                           random_state=0, shuffle=False)\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=2,\n",
    "                             random_state=0)\n",
    "clf.fit(X, y)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(clf.feature_importances_)\n",
    "\n",
    "print(clf.predict([[0, 0, 0, 0]]))\n",
    "# print(clf.predict([[0, 0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import interp\n",
    "\n",
    "from  sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "def class_report(y_true, y_pred, y_score=None, average='micro'):\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        print(\"Error! y_true %s is not the same shape as y_pred %s\" % (\n",
    "              y_true.shape,\n",
    "              y_pred.shape)\n",
    "        )\n",
    "        return\n",
    "\n",
    "    lb = LabelBinarizer()\n",
    "\n",
    "    if len(y_true.shape) == 1:\n",
    "        lb.fit(y_true)\n",
    "\n",
    "    #Value counts of predictions\n",
    "    labels, cnt = np.unique(\n",
    "        y_pred,\n",
    "        return_counts=True)\n",
    "    n_classes = len(labels)\n",
    "    pred_cnt = pd.Series(cnt, index=labels)\n",
    "\n",
    "    metrics_summary = precision_recall_fscore_support(\n",
    "            y_true=y_true,\n",
    "            y_pred=y_pred,\n",
    "            labels=labels)\n",
    "\n",
    "    avg = list(precision_recall_fscore_support(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred,\n",
    "            average='weighted'))\n",
    "\n",
    "    metrics_sum_index = ['precision', 'recall', 'f1-score', 'support']\n",
    "    class_report_df = pd.DataFrame(\n",
    "        list(metrics_summary),\n",
    "        index=metrics_sum_index,\n",
    "        columns=labels)\n",
    "\n",
    "    support = class_report_df.loc['support']\n",
    "    total = support.sum() \n",
    "    class_report_df['avg / total'] = avg[:-1] + [total]\n",
    "\n",
    "    class_report_df = class_report_df.T\n",
    "    class_report_df['pred'] = pred_cnt\n",
    "    class_report_df['pred'].iloc[-1] = total\n",
    "\n",
    "    if not (y_score is None):\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        for label_it, label in enumerate(labels):\n",
    "            fpr[label], tpr[label], _ = roc_curve(\n",
    "                (y_true == label).astype(int), \n",
    "                y_score[:, label_it])\n",
    "\n",
    "            roc_auc[label] = auc(fpr[label], tpr[label])\n",
    "\n",
    "        if average == 'micro':\n",
    "            if n_classes <= 2:\n",
    "                fpr[\"avg / total\"], tpr[\"avg / total\"], _ = roc_curve(\n",
    "                    lb.transform(y_true).ravel(), \n",
    "                    y_score[:, 1].ravel())\n",
    "            else:\n",
    "                fpr[\"avg / total\"], tpr[\"avg / total\"], _ = roc_curve(\n",
    "                        lb.transform(y_true).ravel(), \n",
    "                        y_score.ravel())\n",
    "\n",
    "            roc_auc[\"avg / total\"] = auc(\n",
    "                fpr[\"avg / total\"], \n",
    "                tpr[\"avg / total\"])\n",
    "\n",
    "        elif average == 'macro':\n",
    "            # First aggregate all false positive rates\n",
    "            all_fpr = np.unique(np.concatenate([\n",
    "                fpr[i] for i in labels]\n",
    "            ))\n",
    "\n",
    "            # Then interpolate all ROC curves at this points\n",
    "            mean_tpr = np.zeros_like(all_fpr)\n",
    "            for i in labels:\n",
    "                mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "            # Finally average it and compute AUC\n",
    "            mean_tpr /= n_classes\n",
    "\n",
    "            fpr[\"macro\"] = all_fpr\n",
    "            tpr[\"macro\"] = mean_tpr\n",
    "\n",
    "            roc_auc[\"avg / total\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "        class_report_df['AUC'] = pd.Series(roc_auc)\n",
    "\n",
    "    return class_report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\awannaphasch2016\\AppData\\Local\\Continuum\\anaconda3\\envs\\pytorch_python3.7\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=5000, n_features=10,\n",
    "                           n_informative=5, n_redundant=0,\n",
    "                           n_classes=10, random_state=0, \n",
    "                           shuffle=False)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "model = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.274336  0.678832  0.390756       137\n",
      "           1   0.270270  0.166667  0.206186       120\n",
      "           2   0.442623  0.223140  0.296703       121\n",
      "           3   0.363636  0.310345  0.334884       116\n",
      "           4   0.376147  0.299270  0.333333       137\n",
      "           5   0.727273  0.067797  0.124031       118\n",
      "           6   0.437956  0.400000  0.418118       150\n",
      "           7   0.450000  0.225000  0.300000       120\n",
      "           8   0.222222  0.378378  0.280000       111\n",
      "           9   0.298246  0.425000  0.350515       120\n",
      "\n",
      "    accuracy                       0.324000      1250\n",
      "   macro avg   0.386271  0.317443  0.303453      1250\n",
      "weighted avg   0.386604  0.324000  0.308148      1250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sk_report = classification_report(\n",
    "    digits=6,\n",
    "    y_true=y_test, \n",
    "    y_pred=model.predict(X_test))\n",
    "print(sk_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score  support    pred       AUC\n",
      "0             0.274336  0.678832  0.390756    137.0   339.0  0.780651\n",
      "1             0.270270  0.166667  0.206186    120.0    74.0  0.761409\n",
      "2             0.442623  0.223140  0.296703    121.0    61.0  0.791068\n",
      "3             0.363636  0.310345  0.334884    116.0    99.0  0.840475\n",
      "4             0.376147  0.299270  0.333333    137.0   109.0  0.774778\n",
      "5             0.727273  0.067797  0.124031    118.0    11.0  0.676386\n",
      "6             0.437956  0.400000  0.418118    150.0   137.0  0.842139\n",
      "7             0.450000  0.225000  0.300000    120.0    60.0  0.790302\n",
      "8             0.222222  0.378378  0.280000    111.0   189.0  0.779354\n",
      "9             0.298246  0.425000  0.350515    120.0   171.0  0.796010\n",
      "avg / total   0.386604  0.324000  0.308148   1250.0  1250.0  0.783347\n"
     ]
    }
   ],
   "source": [
    "report_with_auc = class_report(\n",
    "    y_true=y_test, \n",
    "    y_pred=model.predict(X_test), \n",
    "    y_score=model.predict_proba(X_test))\n",
    "\n",
    "print(report_with_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "# First create some toy data:\n",
    "x = np.linspace(0, 2*np.pi, 400)\n",
    "y = np.sin(x**2)\n",
    "\n",
    "# Creates just a figure and only one subplot\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y)\n",
    "ax.set_title('Simple plot')\n",
    "\n",
    "# Creates two subplots and unpacks the output array immediately\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "ax1.plot(x, y)\n",
    "ax1.set_title('Sharing Y axis')\n",
    "ax2.scatter(x, y)\n",
    "\n",
    "# Creates four polar axes, and accesses them through the returned array\n",
    "fig, axes = plt.subplots(2, 2, subplot_kw=dict(polar=True))\n",
    "axes[0, 0].plot(x, y)\n",
    "axes[1, 1].scatter(x, y)\n",
    "\n",
    "# Share a X axis with each column of subplots\n",
    "plt.subplots(2, 2, sharex='col')\n",
    "\n",
    "# Share a Y axis with each row of subplots\n",
    "plt.subplots(2, 2, sharey='row')\n",
    "\n",
    "# Share both X and Y axes with all subplots\n",
    "plt.subplots(2, 2, sharex='all', sharey='all')\n",
    "\n",
    "# Note that this is the same as\n",
    "plt.subplots(2, 2, sharex=True, sharey=True)\n",
    "\n",
    "# Creates figure number 10 with a single subplot\n",
    "# and clears it if it already exists.\n",
    "fig, ax=plt.subplots(num=10, clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1,1)\n",
    "axes.plot([0.        , 0.        , 0.12068966, 1.        ], [0., 1., 1., 1.])\n",
    "# axes.plot([0.12068966, 1.        ], [1., 1.])\n",
    "axes.set_xlim(-0.1,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
